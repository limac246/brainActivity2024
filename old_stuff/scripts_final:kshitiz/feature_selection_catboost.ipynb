{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEWWHge2qGuP"
      },
      "source": [
        "In this notebook, we perform feature selection based feature importance from CatBoost. \n",
        "\n",
        "Note that CatBoost has been set to run on GPU to speed up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V7Cjbh0bYrp",
        "outputId": "95043e96-2ad5-404a-a9ed-650ea489289c"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries for evaluation metrics and cross validation\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "# Libraries for ensemble methods considered in the study\n",
        "import catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQvpEKK6YGC-"
      },
      "outputs": [],
      "source": [
        "# for reproducibility of K-fold dataset stratification and catboost model training\n",
        "random_seed = 216 \n",
        "# path to folder where train data after processing with merge_spectrogram_features_n_train_test_split.ipynb is stored\n",
        "data_path = 'data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUCJw7ORjvNX"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evg6hSYEbtfD"
      },
      "outputs": [],
      "source": [
        "# Reading the Train set\n",
        "train = pd.read_parquet(data_path + 'train.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "D10QSvlpwedB",
        "outputId": "d787cba5-4bc5-4fde-afd6-c67728585698"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKrytaJqv2RG",
        "outputId": "c65f9786-37c7-4d7c-87d4-ce16459c756b"
      },
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEPadYMel9hn",
        "outputId": "67f8cf44-dd85-4803-a96a-66753f686281"
      },
      "outputs": [],
      "source": [
        "# Vote columns corresponding to each of the classes\n",
        "vote_cols = train.columns[train.columns.str.endswith('_vote')].tolist()\n",
        "vote_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwILmSnh_1W9",
        "outputId": "c51b00e6-0275-41af-d9d4-a57d93235d16"
      },
      "outputs": [],
      "source": [
        "# Features to be used for training\n",
        "FEATURES = train.columns[10:train.shape[1]].tolist()\n",
        "len(FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcxMdZzpr12p",
        "outputId": "96c5d740-384d-481d-e118-7433bb566261"
      },
      "outputs": [],
      "source": [
        "# sample weights based on total votes\n",
        "weights_total_vote = [min(t/3,1) for t in train.total_votes.tolist()]\n",
        "weights_total_vote = np.array(weights_total_vote)\n",
        "weights_total_vote.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7znzMrhX-_wW"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKPZ0OUNzYBM"
      },
      "outputs": [],
      "source": [
        "def compute_feature_importance(train, features, classes, weights, cv = 5, random_seed = 216):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "    train - Dataset to perform K-fold CV and compute feature importance\n",
        "    features - name of the features\n",
        "    classes - labels for the eeg classification\n",
        "    weights - for weighting of sample importance based on total votes\n",
        "    cv - number of folds to be used in CV\n",
        "    random_seed - set random number for reproducibility\n",
        "    \n",
        "    Output:\n",
        "    dataframe: rows corresponding to each of the feature and columns corresponding \n",
        "               to feature importance based on validation set of kth CV      \n",
        "    \"\"\"\n",
        "  \n",
        "  # for storing feature importance in each of the CV folds\n",
        "  feature_importance = {}\n",
        "\n",
        "  # StratifiedGroupKFold in order to stratify on the expert consensus and separate patient IDs between k folds\n",
        "  sgkf = StratifiedGroupKFold(n_splits=cv, shuffle=True, random_state=random_seed)\n",
        "  for i, (train_index, valid_index) in enumerate(sgkf.split(X = train, y = train.expert_consensus, groups = train.patient_id)):\n",
        "    \n",
        "    # define the training set for the ith fold\n",
        "    X_train = train.loc[train_index,features].values\n",
        "    y_train = train.loc[train_index,classes].values\n",
        "\n",
        "    # Adapted from https://stackoverflow.com/questions/75762712/how-to-train-xgboost-with-probabilities-instead-of-class\n",
        "    # to train using the probability values of each class in the objective function instead of expert_consensus\n",
        "    n_samples, n_classes = y_train.shape\n",
        "    X_train_upsampled = X_train.repeat(n_classes, axis=0)\n",
        "    y_train_direct = np.tile(range(n_classes), n_samples)\n",
        "    sample_weights = (y_train * np.repeat(weights[train_index],6).reshape(len(weights[train_index]),6)).ravel()\n",
        "\n",
        "    # Define Catboost classifier\n",
        "    clf = catboost.CatBoostClassifier(task_type='GPU',\n",
        "                                      objective='MultiClass',\n",
        "                                      random_state=random_seed,\n",
        "                                      verbose=False)\n",
        "\n",
        "    # fit the model\n",
        "    clf.fit(X_train_upsampled, y_train_direct, sample_weight=sample_weights)\n",
        "\n",
        "    # define the validation set for the ith fold\n",
        "    X_val = train.loc[valid_index,features].values\n",
        "    y_val = train.loc[valid_index,classes].values\n",
        "\n",
        "    # Mapping classes to integer labels\n",
        "    TARGETS = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\n",
        "\n",
        "    # Pool data structure from catboost for the validation data\n",
        "    val_pool = catboost.Pool(\n",
        "        data = train.loc[valid_index,features],\n",
        "        label = train.loc[valid_index,'expert_consensus'].map(TARGETS),\n",
        "    )\n",
        "\n",
        "    # Compute the feature importance using the validation data\n",
        "    feature_importance['CV'+str(i+1)] = clf.get_feature_importance(val_pool)\n",
        "\n",
        "    del X_train, y_train, X_train_upsampled, y_train_direct, X_val, y_val, clf, val_pool, TARGETS\n",
        "\n",
        "  return pd.DataFrame(feature_importance, index = features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoShSYyEUNLb"
      },
      "source": [
        "## CatBoost Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2FbkDBxU1WC",
        "outputId": "9879a152-26e7-47ed-c043-dbdc753f180b"
      },
      "outputs": [],
      "source": [
        "df_feature_importance = compute_feature_importance(train, FEATURES, vote_cols, weights_total_vote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sZtH_FPlCok0",
        "outputId": "3336be35-2c10-4599-846c-e03cfd4ac6c5"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUMZDWG799dl"
      },
      "outputs": [],
      "source": [
        "# average featue importance across 5 folds\n",
        "df_feature_importance['mean_feature_imp'] = df_feature_importance.mean(axis=1)\n",
        "# sort by feature importance\n",
        "df_feature_importance.sort_values(by='mean_feature_imp', inplace=True, ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vyLXdCB-W7T"
      },
      "outputs": [],
      "source": [
        "# cumulative importance\n",
        "df_feature_importance['cumulative_imp'] = df_feature_importance.mean_feature_imp.cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the feature importance scores\n",
        "df_feature_importance.to_parquet(data_path + 'feature_importance.parquet', compression='gzip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select feature corresponding to 90% of the feature importance\n",
        "selected_features = df_feature_importance.iloc[np.where(~(df_feature_importance['cumulative_imp']>=90))[0],:].index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reading the test set\n",
        "test = pd.read_parquet(data_path + 'test.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dimensions of train set: \", train.shape)\n",
        "print(\"Dimensions of test set: \", test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep only the selected features in the train and test sets\n",
        "train_selected = pd.concat([train.iloc[:,:10], train[selected_features]], axis = 1)\n",
        "test_selected = pd.concat([test.iloc[:,:10], test[selected_features]], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dimensions of train set after feature selection: \", train_selected.shape)\n",
        "print(\"Dimensions of test set after feature selection: \", test_selected.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the train and test sets with only the selected features\n",
        "train_selected.to_parquet(data_path + 'train_selected_features.parquet', compression='gzip')\n",
        "test_selected.to_parquet(data_path + 'test_selected_features.parquet', compression='gzip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "C3uIYTgY8L38"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
