{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Creating train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset containing the extracted features from and saved in extract_time_frequency_univariate_features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../train_with_extracted_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106800, 464)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing cases with NAN in extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to possible NANs in the extracted features due to NANs in the processed eeg signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['na_count'] = train.isna().sum(axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of rows with NANs in extracted features\n",
    "idx_na = train.index[train['na_count'] > 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the cases with NANs in extracted features\n",
    "train_filtered = train.drop(index=idx_na).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103256, 464)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seizure_vote',\n",
       " 'lpd_vote',\n",
       " 'gpd_vote',\n",
       " 'lrda_vote',\n",
       " 'grda_vote',\n",
       " 'other_vote']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote_cols = [c for c in train_filtered.columns if '_vote' in c]\n",
    "vote_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select rows with non-overlapping 10s eeg windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this to avoid over representing data points with multiple time offsets close to one another. To achieve this, we first bin the offset seconds into 10s bins, and then select the first row in every other bin, thus ensuring the offset seconds are at least 10s apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered['offset_bins'] = pd.cut(train_filtered['eeg_label_offset_seconds'], bins=[i*10 - 1 for i in range(339)])\n",
    "train_filtered['offset_bins'] = train_filtered['offset_bins'].astype('str')\n",
    "train_filtered['offset_bins'] = train_filtered['offset_bins'].apply(lambda x : (int(x.split(',')[1][1:-1]) + 1) // 10 - 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up expert votes within offset_bins for each eeg_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_ids = train_filtered[['eeg_id','offset_bins','seizure_vote']].groupby(['eeg_id','offset_bins']).seizure_vote.agg('sum').index.get_level_values('eeg_id')\n",
    "offsets = train_filtered[['eeg_id','offset_bins','seizure_vote']].groupby(['eeg_id','offset_bins']).seizure_vote.agg('sum').index.get_level_values('offset_bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eeg_ids)):\n",
    "    # get the all the rows for (eeg_id, offset_bins)\n",
    "    vote_sum = train_filtered.loc[((train_filtered['eeg_id'] == eeg_ids[i]) & (train_filtered['offset_bins'] == offsets[i])),:].sum(axis=0)\n",
    "    \n",
    "    # iterate over rows to change\n",
    "    for idx in train_filtered.index[((train_filtered['eeg_id'] == eeg_ids[i]) & (train_filtered['offset_bins'] == offsets[i]))]:\n",
    "        train_filtered.loc[idx,vote_cols] = vote_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure vote count is same within offset_bins for each vote category\n",
    "for cat in vote_cols:\n",
    "    print(((train_filtered[['eeg_id','offset_bins',cat]].groupby(['eeg_id','offset_bins'])[cat].agg('max') - train_filtered[['eeg_id','offset_bins',cat]].groupby(['eeg_id','offset_bins'])[cat].agg('min')) > 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding votes from the odd offset_bins to prior even bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in set(eeg_ids):\n",
    "    # check if number of offset_bins for eeg_id are greater than 1\n",
    "    if eeg_ids.value_counts()[id] > 1:\n",
    "        # get the offset_bins\n",
    "        id_offsets = offsets[np.where(eeg_ids == id)[0]]\n",
    "        # iterate through odd offset_bins\n",
    "        for i in range(len(id_offsets)):\n",
    "            # had to do this instead of just iterating through odd indices because there are \n",
    "            # cases with even offset bin following an even offset bin \n",
    "            # (eg: eeg_id = 2428433259, 40 followed by 46 causing 47 to be at even index)\n",
    "            if id_offsets[i]%2 == 1:\n",
    "                # get the vote count for the odd offset_bin\n",
    "                votes_to_add = train_filtered.loc[((train_filtered['eeg_id'] == id) & (train_filtered['offset_bins'] == id_offsets[i])),vote_cols].iloc[0,:]\n",
    "                # add to each row of prior even offset_bins\n",
    "                for idx in train_filtered.index[((train_filtered['eeg_id'] == id) & (train_filtered['offset_bins'] == id_offsets[i-1]))]:\n",
    "                    train_filtered.loc[idx,vote_cols] += votes_to_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove overlap cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered = train_filtered.loc[train_filtered['offset_bins'] % 2 == 0]\n",
    "train_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered = train_filtered.groupby(['eeg_id', 'offset_bins']).agg('first').reset_index()\n",
    "train_filtered.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total number of expert votes, and normalize votes to percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered['total_votes'] = train_filtered[vote_cols].sum(axis=1)\n",
    "for c in vote_cols:\n",
    "    train_filtered[c] = train_filtered[c] / train_filtered['total_votes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the features extracted from 10 min Kaggle and 50 sec EEG Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 10 min Kaggle spectrogram extracted features\n",
    "kaggle_spec = pd.read_parquet(\"train_features_from_kaggle_spec.parquet\")\n",
    "# load the 50 sec EEG spectrogram extracted features\n",
    "eeg_spec = pd.read_parquet(\"train_features_from_eeg_spec.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the eeg spectrogram extracted features with dataset containing univariate extracted features\n",
    "df = train_filtered.merge(right=eeg_spec, on=['eeg_id','eeg_label_offset_seconds'], how = 'left').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the kaggle spectrogram extracted features with dataset containing univariate extracted features and 50sec eeg spectrogram features\n",
    "df = train_filtered.merge(right=kaggle_spec, on=['spectrogram_id','spectrogram_label_offset_seconds'], how = 'left').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform train test split stratified on expert consensus. Additionally, we make sure that the patient IDs in train and test sets are disjoint, and that the test set does not contain any row with fewer than 3 expert votes.\n",
    "\n",
    "We use the first split of StratifiedGroupKFold in order to stratify on the expert consensus and separate patient IDs between the train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First seaparate out the entries with total_votes == 1 | total_votes == 2, considered to be as weak samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_low_total_votes = df[df['total_votes'] <= 2]\n",
    "df_low_total_votes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high = df[df['total_votes'] > 2]\n",
    "df_high.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the set with total_votes >=3 for train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgkf = StratifiedGroupKFold(n_splits=10, shuffle=True, random_state=216)\n",
    "for (t,v) in sgkf.split(X = df_high, y=df_high['expert_consensus'], groups=df_high['patient_id']):\n",
    "    train_idx_full = t\n",
    "    test_idx_full = v\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_train = df_high.iloc[train_idx_full]\n",
    "df_high_test = df_high.iloc[test_idx_full]\n",
    "print(df_high_train.shape, df_high_test.shape, df_high.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that none of patient IDs with total votes < 3 are in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpids = df_high_test['patient_id'].values.tolist()\n",
    "df_low_total_votes = df_low_total_votes[ ~df_low_total_votes['patient_id'].isin(bpids) ]\n",
    "set(df_low_total_votes.patient_id.values).intersection(set(df_high_test.patient_id.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add back the rows with low total vote counts to only the train set, still keeping the patient IDs in the two sets disjoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_low_total_votes, df_high_train])\n",
    "df_test = df_high_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sort_index(inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['expert_consensus'].value_counts(normalize=True), '\\n')\n",
    "print(df_train['expert_consensus'].value_counts(normalize=True), '\\n')\n",
    "print(df_test['expert_consensus'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = pd.DataFrame(df['expert_consensus'].value_counts(normalize=True))\n",
    "tmp1.reset_index(inplace=True)\n",
    "\n",
    "tmp2 = pd.DataFrame(df_train['expert_consensus'].value_counts(normalize=True))\n",
    "tmp2.reset_index(inplace=True)\n",
    "\n",
    "tmp3 = pd.DataFrame(df_test['expert_consensus'].value_counts(normalize=True))\n",
    "tmp3.reset_index(inplace=True)\n",
    "\n",
    "tmp1 = tmp1.sort_values(by='expert_consensus')\n",
    "tmp2 = tmp2.sort_values(by='expert_consensus')\n",
    "tmp3 = tmp3.sort_values(by='expert_consensus')\n",
    "\n",
    "X = list(tmp1['expert_consensus'].values)\n",
    "\n",
    "Y1 = tmp1['proportion']\n",
    "Y2 = tmp2['proportion']\n",
    "Y3 = tmp3['proportion']\n",
    "\n",
    "fig, axs = plt.subplots(3, figsize=(8,10))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "axs[0].bar(X, Y1)\n",
    "axs[0].set_title('Full data set')\n",
    "\n",
    "axs[1].bar(X, Y2)\n",
    "axs[1].set_title(f'Train set - {(df_train.shape[0] / df.shape[0]) * 100 : .2f}% of data')\n",
    "\n",
    "axs[2].bar(X, Y3)\n",
    "axs[2].set_title(f'Test set- {(df_test.shape[0] / df.shape[0]) * 100 : .2f}% of data')\n",
    "\n",
    "for i, p in enumerate(Y1):\n",
    "    axs[0].text(i, p, f'{p*100 : .2f}%', ha='center', va='bottom')\n",
    "\n",
    "for i, p in enumerate(Y2):\n",
    "    axs[1].text(i, p, f'{p*100 : .2f}%', ha='center', va='bottom')\n",
    "\n",
    "for i, p in enumerate(Y3):\n",
    "    axs[2].text(i, p, f'{p*100 : .2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet('data/train.parquet', index = False, compression = 'gzip')\n",
    "df_test.to_parquet('data/test.parquet', index = False, compression = 'gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
