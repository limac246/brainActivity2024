{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import rel_entr\n",
    "\n",
    "def kl_divergence(solution: np.ndarray, submission: np.ndarray, epsilon: float=10**-15, micro_average: bool=True, sample_weights: pd.Series =None):\n",
    "    \n",
    "     # Prevent issue with populating int columns with floats\n",
    "        \n",
    "    solution= solution.astype(float)\n",
    "\n",
    "        \n",
    "    # clipping the min prevents users from playing games with the 20th decimal place of predictions.\n",
    "    submission = np.clip(submission, epsilon, 1 )\n",
    "\n",
    "    if micro_average:\n",
    "        return np.average(rel_entr(solution, submission).sum(axis=1), weights=sample_weights)\n",
    "    else:\n",
    "        return np.average(rel_entr(solution, submission).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the train dataframe with minimal filtering and no features\n",
    "df=pd.read_csv('../../train_final_less_filtering.csv')\n",
    "\n",
    "#dataframe with eeg-derived features, \n",
    "#setting index of the one saved in train.csv to the one in train_final_less_filtering.csv\n",
    "#to allow fast dropping of the row in spec derived feature file\n",
    "df_train=pd.read_csv('../../features_folder/merged_votes_no_overlap_filter_before_split/train.csv',index_col=0)\n",
    "df_train=df[['Unnamed: 0', 'label_id']].merge(df_train, how='inner', left_on='label_id', right_on='label_id')\n",
    "df_train=df_train.set_index('Unnamed: 0')\n",
    "df_train.index.name=None\n",
    "\n",
    "#dataframe with kaggle spec derived features\n",
    "df_train2=pd.read_parquet('../../train_features_from_kaggle_spec.parquet').loc[df_train.index]\n",
    "\n",
    "#dataframe with spec from eeg features\n",
    "df_train3=pd.read_parquet('../../features_folder/features_from_eeg_spectrograms_without_kaggle_spec_data/train_mrgd_votes_no_ovlp_bfsplt_feats_from_eeg_to_spec.parquet')\n",
    "df_train3=df[['Unnamed: 0', 'label_id']].merge(df_train3, how='inner', left_on='label_id', right_on='label_id')\n",
    "df_train3=df_train3.set_index('Unnamed: 0')\n",
    "df_train3.index.name=None\n",
    "df_train3=df_train3[df_train3.columns[465:]]\n",
    "\n",
    "\n",
    "#not all yet>>merging all features together\n",
    "df_train=df_train.join(other=df_train2[df_train2.columns[3:]], how='left')\n",
    "df_train=df_train.join(other=df_train3, how='left')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some rows have NaN values - these come from kaggle spec-derived features. Total 30\n",
    "df.set_index('Unnamed: 0').loc[df_train[df_train.isna().sum(axis=1)>0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features names. all features from kaggle spec and eeg \n",
    "feature_cols=list(df_train.columns[17:])\n",
    "#features from kaggle spec\n",
    "spec_features=df_train2.columns[3:]\n",
    "#features from eeg\n",
    "eeg_features=feature_cols[:448]\n",
    "#features from specs from eegs\n",
    "mels_features=df_train3.columns\n",
    "#columns with probability values\n",
    "vote_cats=['seizure_vote', 'lpd_vote',\t'gpd_vote',\t'lrda_vote',\t'grda_vote',\t'other_vote']\n",
    "# encoding of the expert consensus into integers\n",
    "codes = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features=['T5-O1.skewness',\n",
    " 'F4-C4.samp_en',\n",
    " 'C4-P4.samp_en',\n",
    " 'T5-O1.rel_bp_delta',\n",
    " 'C4-P4.kurtosis',\n",
    " 'Fp1-F3.abs_bp_delta',\n",
    " 'LP_10.16_mean_20s',\n",
    " 'F3-C3.bp_delta_theta',\n",
    " 'F3-C3.rel_bp_delta',\n",
    " 'T5-O1.abs_bp_beta',\n",
    " 'F4-C4.abs_bp_delta',\n",
    " 'T6-O2.skewness',\n",
    " 'RL_1.56_min_10m',\n",
    " 'T5-O1.bp_delta_beta',\n",
    " 'eeg_std_f302_10s',\n",
    " 'P4-O2.samp_en',\n",
    " 'F4-C4.abs_bp_theta',\n",
    " 'RP_19.92_min_10m',\n",
    " 'T4-T6.rel_bp_theta',\n",
    " 'LP_3.52_min_10m',\n",
    " 'C4-P4.rel_bp_delta',\n",
    " 'T3-T5.rel_bp_delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression on all features \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "\n",
    "features=list(mels_features)+list(eeg_features)+list(spec_features)\n",
    "\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "\n",
    "    n_classes = 6\n",
    "\n",
    "    print(f'running fold {i}')\n",
    "    X_train=df_train.iloc[train_index].dropna()[features]\n",
    "    Y_train=df_train.iloc[train_index].dropna().expert_consensus.map(codes)\n",
    "    Y_train_proba=df_train.iloc[train_index].dropna()[vote_cats]\n",
    "\n",
    "    n_samples, n_classes=Y_train_proba.shape\n",
    "    X_upsampled = np.array(X_train).repeat(n_classes, axis=0)\n",
    "    Y_direct = np.tile(range(n_classes), n_samples)\n",
    "    \n",
    "    weight=np.array(df_train.iloc[train_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    sample_weight = np.array(Y_train_proba).ravel()*weight.repeat(6, axis=0)\n",
    "    \n",
    "    X_test=df_train.iloc[test_index].dropna()[features]\n",
    "    Y_test_cat=df_train.iloc[test_index].dropna().expert_consensus.map(codes)\n",
    "    Y_test_proba=df_train.iloc[test_index].dropna()[vote_cats]\n",
    "\n",
    "    pipel=Pipeline([('scale', StandardScaler()), ('logreg',  LogisticRegression(max_iter=1000))])\n",
    "    pipel.fit(X_upsampled,Y_direct, **{'logreg__sample_weight': sample_weight})\n",
    "    pred=pipel.predict_proba(X_test.values)\n",
    "\n",
    "    weight=np.array(df_train.iloc[test_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred, sample_weights=weight)\n",
    "    df_scores.loc[i,'accuracy']=pipel.score(X_test.values, Y_test_cat.values)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression on kaggle spec+ mels spec features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "\n",
    "features=list(mels_features)+list(spec_features)\n",
    "\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "\n",
    "    n_classes = 6\n",
    "\n",
    "    print(f'running fold {i}')\n",
    "    X_train=df_train.iloc[train_index].dropna()[features]\n",
    "    Y_train=df_train.iloc[train_index].dropna().expert_consensus.map(codes)\n",
    "    Y_train_proba=df_train.iloc[train_index].dropna()[vote_cats]\n",
    "\n",
    "    n_samples, n_classes=Y_train_proba.shape\n",
    "    X_upsampled = np.array(X_train).repeat(n_classes, axis=0)\n",
    "    Y_direct = np.tile(range(n_classes), n_samples)\n",
    "    \n",
    "    weight=np.array(df_train.iloc[train_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    sample_weight = np.array(Y_train_proba).ravel()*weight.repeat(6, axis=0)\n",
    "    \n",
    "    X_test=df_train.iloc[test_index].dropna()[features]\n",
    "    Y_test_cat=df_train.iloc[test_index].dropna().expert_consensus.map(codes)\n",
    "    Y_test_proba=df_train.iloc[test_index].dropna()[vote_cats]\n",
    "\n",
    "    pipel=Pipeline([('scale', StandardScaler()), ('logreg',  LogisticRegression(penalty=None,max_iter=1000))])\n",
    "    pipel.fit(X_upsampled,Y_direct, **{'logreg__sample_weight': sample_weight})\n",
    "    pred=pipel.predict_proba(X_test.values)\n",
    "\n",
    "    weight=np.array(df_train.iloc[test_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred, sample_weights=weight)\n",
    "    df_scores.loc[i,'accuracy']=pipel.score(X_test.values, Y_test_cat.values)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression on select features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "\n",
    "features=select_features\n",
    "\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "\n",
    "    n_classes = 6\n",
    "\n",
    "    print(f'running fold {i}')\n",
    "    X_train=df_train.iloc[train_index].dropna()[features]\n",
    "    Y_train=df_train.iloc[train_index].dropna().expert_consensus.map(codes)\n",
    "    Y_train_proba=df_train.iloc[train_index].dropna()[vote_cats]\n",
    "\n",
    "    n_samples, n_classes=Y_train_proba.shape\n",
    "    X_upsampled = np.array(X_train).repeat(n_classes, axis=0)\n",
    "    Y_direct = np.tile(range(n_classes), n_samples)\n",
    "    \n",
    "    weight=np.array(df_train.iloc[train_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    sample_weight = np.array(Y_train_proba).ravel()*weight.repeat(6, axis=0)\n",
    "    \n",
    "    X_test=df_train.iloc[test_index].dropna()[features]\n",
    "    Y_test_cat=df_train.iloc[test_index].dropna().expert_consensus.map(codes)\n",
    "    Y_test_proba=df_train.iloc[test_index].dropna()[vote_cats]\n",
    "\n",
    "    pipel=Pipeline([('scale', StandardScaler()), ('logreg',  LogisticRegression(max_iter=1000))])\n",
    "    pipel.fit(X_upsampled,Y_direct, **{'logreg__sample_weight': sample_weight})\n",
    "    pred=pipel.predict_proba(X_test.values)\n",
    "\n",
    "    weight=np.array(df_train.iloc[test_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred, sample_weights=weight)\n",
    "    df_scores.loc[i,'accuracy']=pipel.score(X_test.values, Y_test_cat.values)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes eeg+kaggle spec features\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "features=list(eeg_features)+list(spec_features)\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "    n_classes=6\n",
    "    print(f'running fold {i}')\n",
    "    X_train=df_train.iloc[train_index].dropna()[features]\n",
    "    Y_train=df_train.iloc[train_index].dropna().expert_consensus.map(codes)\n",
    "    Y_train_proba=df_train.iloc[train_index].dropna()[vote_cats]\n",
    "\n",
    "    n_samples, n_classes=Y_train_proba.shape\n",
    "    X_upsampled = np.array(X_train).repeat(n_classes, axis=0)\n",
    "    Y_direct = np.tile(range(n_classes), n_samples)\n",
    "    \n",
    "    weight=np.array(df_train.iloc[train_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    sample_weight = np.array(Y_train_proba).ravel()*weight.repeat(6, axis=0)\n",
    "    \n",
    "    X_test=df_train.iloc[test_index].dropna()[features]\n",
    "    Y_test_cat=df_train.iloc[test_index].dropna().expert_consensus.map(codes)\n",
    "    Y_test_proba=df_train.iloc[test_index].dropna()[vote_cats]\n",
    "\n",
    "    pipe=Pipeline([ ('naive',  GaussianNB())])\n",
    "    pipe.fit(X_upsampled,Y_direct, **{'naive__sample_weight': sample_weight})\n",
    "    pred=pipe.predict_proba(X_test.values)\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred)\n",
    "    df_scores.loc[i,'accuracy']=pipe.score(X_test.values, Y_test_cat.values)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes on kaggle spec +mels features\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "features=list(mels_features)+list(spec_features)\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "    n_classes=6\n",
    "    print(f'running fold {i}')\n",
    "    X_train=df_train.iloc[train_index].dropna()[features]\n",
    "    Y_train=df_train.iloc[train_index].dropna().expert_consensus.map(codes)\n",
    "    Y_train_proba=df_train.iloc[train_index].dropna()[vote_cats]\n",
    "\n",
    "    n_samples, n_classes=Y_train_proba.shape\n",
    "    X_upsampled = np.array(X_train).repeat(n_classes, axis=0)\n",
    "    Y_direct = np.tile(range(n_classes), n_samples)\n",
    "    \n",
    "    weight=np.array(df_train.iloc[train_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    sample_weight = np.array(Y_train_proba).ravel()*weight.repeat(6, axis=0)\n",
    "    \n",
    "    X_test=df_train.iloc[test_index].dropna()[features]\n",
    "    Y_test_cat=df_train.iloc[test_index].dropna().expert_consensus.map(codes)\n",
    "    Y_test_proba=df_train.iloc[test_index].dropna()[vote_cats]\n",
    "\n",
    "    pipe=Pipeline([ ('naive',  GaussianNB())])\n",
    "    pipe.fit(X_upsampled,Y_direct,**{'naive__sample_weight': sample_weight})\n",
    "    pred=pipe.predict_proba(X_test.values)\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred)\n",
    "    df_scores.loc[i,'accuracy']=pipe.score(X_test.values, Y_test_cat.values)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes select features \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "features=select_features\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "    n_classes=6\n",
    "    print(f'running fold {i}')\n",
    "    X_train=df_train.iloc[train_index].dropna()[features]\n",
    "    Y_train=df_train.iloc[train_index].dropna().expert_consensus.map(codes)\n",
    "    Y_train_proba=df_train.iloc[train_index].dropna()[vote_cats]\n",
    "\n",
    "    n_samples, n_classes=Y_train_proba.shape\n",
    "    X_upsampled = np.array(X_train).repeat(n_classes, axis=0)\n",
    "    Y_direct = np.tile(range(n_classes), n_samples)\n",
    "    \n",
    "    weight=np.array(df_train.iloc[train_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "    sample_weight = np.array(Y_train_proba).ravel()*weight.repeat(6, axis=0)\n",
    "    \n",
    "    X_test=df_train.iloc[test_index].dropna()[features]\n",
    "    Y_test_cat=df_train.iloc[test_index].dropna().expert_consensus.map(codes)\n",
    "    Y_test_proba=df_train.iloc[test_index].dropna()[vote_cats]\n",
    "\n",
    "    pipe=Pipeline([ ('naive',  GaussianNB())])\n",
    "    pipe.fit(X_upsampled,Y_direct, **{'naive__sample_weight': sample_weight})\n",
    "    pred=pipe.predict_proba(X_test.values)\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred)\n",
    "    df_scores.loc[i,'accuracy']=pipe.score(X_test.values, Y_test_cat.values)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes on select features trained on expert consensus\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "features=select_features\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "    print(f'running fold {i}')\n",
    "    X_train=df_train.iloc[train_index].dropna()[features]\n",
    "    Y_train=df_train.iloc[train_index].dropna().expert_consensus.map(codes)\n",
    "    X_test=df_train.iloc[test_index].dropna()[features]\n",
    "    Y_test_cat=df_train.iloc[test_index].dropna().expert_consensus.map(codes)\n",
    "    Y_test_proba=df_train.iloc[test_index].dropna()[vote_cats]\n",
    "\n",
    "    weight=np.array(df_train.iloc[train_index].dropna().total_votes.apply(lambda x: min(x/3, 1.0)))\n",
    "\n",
    "    pipe=Pipeline([ ('naive',  GaussianNB())])\n",
    "    pipe.fit(X_train,Y_train, **{'naive__sample_weight': weight})\n",
    "    pred=pipe.predict_proba(X_test)\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred, )\n",
    "    df_scores.loc[i,'accuracy']=pipe.score(X_test, Y_test_cat)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model: predicting probability 1/6 for each category\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "features=feature_cols\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "    print(f'running fold {i}')\n",
    "\n",
    "    Y_test_proba=df_train.iloc[test_index][vote_cats]\n",
    "    pred=np.full(Y_test_proba.shape, 1/6, dtype=float)\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline model: predicting probability \n",
    "# Other      0.29\n",
    "# Seizure    0.23\n",
    "# GRDA       0.15\n",
    "# LPD        0.15\n",
    "# GPD        0.11\n",
    "# LRDA       0.07\n",
    "#according to the data distribution\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer, FeatureUnion\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "\n",
    "n_splits=5\n",
    "sgkf=StratifiedGroupKFold(n_splits=n_splits, random_state=216, shuffle=True)\n",
    "gen=sgkf.split(X=df_train, y=df_train['expert_consensus'], \n",
    "                        groups=df_train['patient_id'])\n",
    "features=feature_cols\n",
    "df_scores=pd.DataFrame(index=list(range(n_splits)), columns=['kl_div', 'accuracy'], data=None)\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gen):\n",
    "    print(f'running fold {i}')\n",
    "\n",
    "    Y_test_proba=df_train.iloc[test_index][vote_cats]\n",
    "    pred=np.full(Y_test_proba.shape, [0.23, 0.15, 0.11, 0.07, 0.15, 0.29], dtype=float)\n",
    "    df_scores.loc[i,'kl_div']=kl_divergence(Y_test_proba, pred)\n",
    "\n",
    "display(df_scores)\n",
    "display(df_scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
